{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"uni-adv-nlp.ipynb","provenance":[],"authorship_tag":"ABX9TyOCbWKSbYmXoMYzYFKxazjl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uCldOPi--2bQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"8813ff14-26d1-404c-df65-eaa370a20f37","executionInfo":{"status":"ok","timestamp":1586875998100,"user_tz":0,"elapsed":9100,"user":{"displayName":"maulik chevli","photoUrl":"","userId":"04927803193533666412"}}},"source":["!git clone https://github.com/Eric-Wallace/universal-triggers.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'universal-triggers'...\n","remote: Enumerating objects: 14, done.\u001b[K\n","remote: Counting objects: 100% (14/14), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 45 (delta 3), reused 3 (delta 0), pack-reused 31\u001b[K\n","Unpacking objects: 100% (45/45), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zU9ifu4v-_Vk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2e4a3481-e0ad-4461-e877-b0359e0f2bac","executionInfo":{"status":"ok","timestamp":1586876165800,"user_tz":0,"elapsed":110400,"user":{"displayName":"maulik chevli","photoUrl":"","userId":"04927803193533666412"}}},"source":["!pip install allennlp"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 396kB/s \n","\u001b[?25hCollecting word2number>=1.1\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Collecting conllu==1.3.1\n","  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n","Collecting parsimonious>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n","Collecting pytorch-transformers==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 57.3MB/s \n","\u001b[?25hCollecting responses>=0.7\n","  Downloading https://files.pythonhosted.org/packages/a5/52/8063322bd9ee6e7921b74fcb730c6ba983ff995ddfabd966bb689e313464/responses-0.10.12-py2.py3-none-any.whl\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.38.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n","Collecting pytorch-pretrained-bert>=0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 53.3MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n","Collecting gevent>=1.3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/95/b53b78b15abbe547bed7381ca9c8319c86d6b646a30d0831e26c307a5fa7/gevent-1.5.0-cp36-cp36m-manylinux2010_x86_64.whl (5.1MB)\n","\u001b[K     |████████████████████████████████| 5.1MB 41.8MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Collecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 49.9MB/s \n","\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n","Collecting numpydoc>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.12.38)\n","Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n","\u001b[K     |████████████████████████████████| 256kB 44.4MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/cb/e0/54421447d55bc7304a785be9ec81f28e1e8a8c6619b0e35154ed8f1b7761/jsonpickle-1.4-py2.py3-none-any.whl\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n","Collecting spacy<2.2,>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n","\u001b[K     |████████████████████████████████| 30.9MB 100kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.2)\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 40.3MB/s \n","\u001b[?25hCollecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 10.5MB/s \n","\u001b[?25hCollecting flaky\n","  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n","Collecting overrides\n","  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n","Collecting flask-cors>=3.0.7\n","  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.1)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.11.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 43.7MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n","Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n","Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.15.38)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (46.1.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n","Collecting preshed<2.1.0,>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n","\u001b[K     |████████████████████████████████| 92kB 14.1MB/s \n","\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n","Collecting thinc<7.1.0,>=7.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 55.0MB/s \n","\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 48.2MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.3)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n","Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n","Building wheels for collected packages: word2number, parsimonious, numpydoc, jsonnet, ftfy, overrides\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=924bf2a5288aa1f3f59c9251e8007c299d3f953c6d612b80ecbc43060288ea22\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=2e5d081ca9a7d4bcc62594af987cb6da0dbe1df123412357f68a1bb484bba872\n","  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n","  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31893 sha256=29695997910b7181d2184aa9dac050ea0d178a8a53d9d8902ef17520fb17a114\n","  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3319809 sha256=b0cd46aef4082d69dd3fdc5d08f5e89bb3ac1e0272a4bfe7e63d2a5dd8de7649\n","  Stored in directory: /root/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=aaa01e828827ba62270db4c74a39707f562c538839457fc25b2dff196cf23771\n","  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5609 sha256=ac0b96a6ee78ff9ac49f62d585d7124bc61f2d25ada42702cd72653481a780e5\n","  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n","Successfully built word2number parsimonious numpydoc jsonnet ftfy overrides\n","\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n","Installing collected packages: word2number, conllu, parsimonious, sentencepiece, pytorch-transformers, responses, pytorch-pretrained-bert, greenlet, gevent, tensorboardX, numpydoc, jsonnet, jsonpickle, preshed, plac, blis, thinc, spacy, unidecode, ftfy, flaky, overrides, flask-cors, allennlp\n","  Found existing installation: preshed 3.0.2\n","    Uninstalling preshed-3.0.2:\n","      Successfully uninstalled preshed-3.0.2\n","  Found existing installation: plac 1.1.3\n","    Uninstalling plac-1.1.3:\n","      Successfully uninstalled plac-1.1.3\n","  Found existing installation: blis 0.4.1\n","    Uninstalling blis-0.4.1:\n","      Successfully uninstalled blis-0.4.1\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 gevent-1.5.0 greenlet-0.4.15 jsonnet-0.15.0 jsonpickle-1.4 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.12 sentencepiece-0.1.85 spacy-2.1.9 tensorboardX-2.0 thinc-7.0.8 unidecode-1.1.1 word2number-1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ob5VEWec_MbE","colab_type":"code","colab":{}},"source":["import os\n","PATH = '/content/universal-triggers/sst'\n","os.chdir(PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPQcc0SwAL7I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"outputId":"486012ed-ab52-4ed7-f0d5-ba56813cb954","executionInfo":{"status":"ok","timestamp":1586887256900,"user_tz":0,"elapsed":75600,"user":{"displayName":"maulik chevli","photoUrl":"","userId":"04927803193533666412"}}},"source":["import sys\n","import os.path\n","from sklearn.neighbors import KDTree\n","import torch\n","import torch.optim as optim\n","from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n","    StanfordSentimentTreeBankDatasetReader\n","from allennlp.data.iterators import BucketIterator, BasicIterator\n","from allennlp.data.vocabulary import Vocabulary\n","from allennlp.models import Model\n","from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","from allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.nn.util import get_text_field_mask\n","from allennlp.training.metrics import CategoricalAccuracy\n","from allennlp.training.trainer import Trainer\n","from allennlp.common.util import lazy_groups_of\n","from allennlp.data.token_indexers import SingleIdTokenIndexer\n","sys.path.append('..')\n","import utils\n","import attacks\n","\n","# Simple LSTM classifier that uses the final hidden state to classify Sentiment. Based on AllenNLP\n","class LstmClassifier(Model):\n","    def __init__(self, word_embeddings, encoder, vocab):\n","        super().__init__(vocab)\n","        self.word_embeddings = word_embeddings\n","        self.encoder = encoder\n","        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n","                                      out_features=vocab.get_vocab_size('labels'))\n","        self.accuracy = CategoricalAccuracy()\n","        self.loss_function = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, tokens, label):\n","        mask = get_text_field_mask(tokens)\n","        embeddings = self.word_embeddings(tokens)\n","        encoder_out = self.encoder(embeddings, mask)\n","        logits = self.linear(encoder_out)\n","        output = {\"logits\": logits}\n","        if label is not None:\n","            self.accuracy(logits, label)\n","            output[\"loss\"] = self.loss_function(logits, label)\n","        return output\n","\n","    def get_metrics(self, reset=False):\n","        return {'accuracy': self.accuracy.get_metric(reset)}\n","\n","EMBEDDING_TYPE = \"w2v\" # what type of word embeddings to use\n","\n","\n","### MAIN\n","# load the binary SST dataset.\n","single_id_indexer = SingleIdTokenIndexer(lowercase_tokens=True) # word tokenizer\n","# use_subtrees gives us a bit of extra data by breaking down each example into sub sentences.\n","reader = StanfordSentimentTreeBankDatasetReader(granularity=\"2-class\",\n","                                                token_indexers={\"tokens\": single_id_indexer},\n","                                                use_subtrees=True)\n","train_data = reader.read('https://s3-us-west-2.amazonaws.com/allennlp/datasets/sst/train.txt')\n","reader = StanfordSentimentTreeBankDatasetReader(granularity=\"2-class\",\n","                                                token_indexers={\"tokens\": single_id_indexer})\n","dev_data = reader.read('https://s3-us-west-2.amazonaws.com/allennlp/datasets/sst/dev.txt')\n","# test_dataset = reader.read('data/sst/test.txt')\n","\n","vocab = Vocabulary.from_instances(train_data)\n","\n","# Randomly initialize vectors\n","if EMBEDDING_TYPE == \"None\":\n","    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=300)\n","    word_embedding_dim = 300\n","\n","# Load word2vec vectors\n","elif EMBEDDING_TYPE == \"w2v\":\n","    embedding_path = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n","    weight = _read_pretrained_embeddings_file(embedding_path,\n","                                                embedding_dim=300,\n","                                                vocab=vocab,\n","                                                namespace=\"tokens\")\n","    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n","                                embedding_dim=300,\n","                                weight=weight,\n","                                trainable=False)\n","    word_embedding_dim = 300\n","\n","# Initialize model, cuda(), and optimizer\n","word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n","encoder = PytorchSeq2VecWrapper(torch.nn.LSTM(word_embedding_dim,\n","                                                hidden_size=512,\n","                                                num_layers=2,\n","                                                batch_first=True))\n","model = LstmClassifier(word_embeddings, encoder, vocab)\n","model.cuda()\n","\n","# where to save the model\n","model_path = \"/tmp/\" + EMBEDDING_TYPE + \"_\" + \"model.th\"\n","vocab_path = \"/tmp/\" + EMBEDDING_TYPE + \"_\" + \"vocab\"\n","# if the model already exists (its been trained), load the pre-trained weights and vocabulary\n","if os.path.isfile(model_path):\n","    vocab = Vocabulary.from_files(vocab_path)\n","    model = LstmClassifier(word_embeddings, encoder, vocab)\n","    with open(model_path, 'rb') as f:\n","        model.load_state_dict(torch.load(f))\n","# otherwise train model from scratch and save its weights\n","else:\n","    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n","    iterator.index_with(vocab)\n","    optimizer = optim.Adam(model.parameters())\n","    trainer = Trainer(model=model,\n","                        optimizer=optimizer,\n","                        iterator=iterator,\n","                        train_dataset=train_data,\n","                        validation_dataset=dev_data,\n","                        num_epochs=5,\n","                        patience=1,\n","                        cuda_device=0)\n","    trainer.train()\n","    with open(model_path, 'wb') as f:\n","        torch.save(model.state_dict(), f)\n","    vocab.save_to_files(vocab_path)\n","model.train().cuda() # rnn cannot do backwards in train mode\n","\n","# Register a gradient hook on the embeddings. This saves the gradient w.r.t. the word embeddings.\n","# We use the gradient later in the attack.\n","utils.add_hooks(model)\n","embedding_weight = utils.get_embedding_weight(model) # also save the word embedding matrix\n","\n","# Use batches of size universal_perturb_batch_size for the attacks.\n","universal_perturb_batch_size = 128\n","iterator = BasicIterator(batch_size=universal_perturb_batch_size)\n","iterator.index_with(vocab)\n","\n","# Build k-d Tree if you are using gradient + nearest neighbor attack\n","# tree = KDTree(embedding_weight.numpy())\n","\n","# filter the dataset to only positive or negative examples\n","# (the trigger will cause the opposite prediction)\n","dataset_label_filter = \"0\"\n","targeted_dev_data = []\n","for instance in dev_data:\n","    if instance['label'].label == dataset_label_filter:\n","        targeted_dev_data.append(instance)\n","\n","# get accuracy before adding triggers\n","utils.get_accuracy(model, targeted_dev_data, vocab, trigger_token_ids=None)\n","model.train() # rnn cannot do backwards in train mode\n","\n","classifier = model\n","\n","# initialize triggers which are concatenated to the input\n","num_trigger_tokens = 3\n","trigger_token_ids = [vocab.get_token_index(\"the\")] * num_trigger_tokens\n","\n","# sample batches, update the triggers, and repeat\n","for batch in lazy_groups_of(iterator(targeted_dev_data, num_epochs=5, shuffle=True), group_size=1):\n","    # get accuracy with current triggers\n","    utils.get_accuracy(model, targeted_dev_data, vocab, trigger_token_ids)\n","    model.train() # rnn cannot do backwards in train mode\n","\n","    # get gradient w.r.t. trigger embeddings for current batch\n","    averaged_grad = utils.get_average_grad(model, batch, trigger_token_ids)\n","\n","    # pass the gradients to a particular attack to generate token candidates for each token.\n","    cand_trigger_token_ids = attacks.hotflip_attack(averaged_grad,\n","                                                    embedding_weight,\n","                                                    trigger_token_ids,\n","                                                    num_candidates=40,\n","                                                    increase_loss=True)\n","    # cand_trigger_token_ids = attacks.random_attack(embedding_weight,\n","    #                                                trigger_token_ids,\n","    #                                                num_candidates=40)\n","    # cand_trigger_token_ids = attacks.nearest_neighbor_grad(averaged_grad,\n","    #                                                        embedding_weight,\n","    #                                                        trigger_token_ids,\n","    #                                                        tree,\n","    #                                                        100,\n","    #                                                        num_candidates=40,\n","    #                                                        increase_loss=True)\n","\n","    # Tries all of the candidates and returns the trigger sequence with highest loss.\n","    trigger_token_ids = utils.get_best_candidates(model,\n","                                                    batch,\n","                                                    trigger_token_ids,\n","                                                    cand_trigger_token_ids)\n","\n","# print accuracy after adding triggers\n","utils.get_accuracy(model, targeted_dev_data, vocab, trigger_token_ids)"],"execution_count":96,"outputs":[{"output_type":"stream","text":["98794it [00:06, 16119.81it/s]\n","872it [00:00, 1103.07it/s]\n","100%|██████████| 98794/98794 [00:00<00:00, 169448.15it/s]\n","100%|██████████| 1999995/1999995 [00:34<00:00, 57593.31it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Without Triggers: 0.9112149532710281\n","Current Triggers: the, the, the,  : 0.9182242990654206\n","Current Triggers: vibrant, vibrant, thought-provoking,  : 0.1939252336448598\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, restored,  : 0.08177570093457943\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, restored,  : 0.08177570093457943\n","Current Triggers: irresistibly, flavorful, restored,  : 0.08177570093457943\n","Current Triggers: irresistibly, flavorful, restored,  : 0.08177570093457943\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n","Current Triggers: irresistibly, flavorful, improves,  : 0.08411214953271028\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vg5U5Q2bmySQ","colab_type":"code","colab":{}},"source":["from allennlp.data.dataset_readers.text_classification_json import TextClassificationJsonReader\n","reader = TextClassificationJsonReader()\n","text = []\n","text.append(reader.text_to_instance(text=\"irresistibly, flavorful, improves,The movie is bad\", label=\"1\"))\n","text.append(reader.text_to_instance(text=\"The movie is bad\", label=\"1\"))\n","text.append(reader.text_to_instance(text=\"irresistibly, flavorful, improves\", label=\"1\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6P5RAv_AQMk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"2fb8b993-f8f9-45f7-af50-99209398a9d1","executionInfo":{"status":"ok","timestamp":1586888018600,"user_tz":0,"elapsed":2400,"user":{"displayName":"maulik chevli","photoUrl":"","userId":"04927803193533666412"}}},"source":["import numpy as np\n","iterator = BasicIterator(batch_size=universal_perturb_batch_size)\n","iterator.index_with(vocab)\n","\n","for i, batch in enumerate(iterator(text, num_epochs=1, shuffle=False)):\n","    batch = move_to_device(batch, cuda_device=0)\n","    ans = classifier(batch['tokens'], batch['label'])\n","    print(ans)"],"execution_count":129,"outputs":[{"output_type":"stream","text":["{'logits': tensor([[ 0.1357, -0.3485],\n","        [-2.3997,  2.4109],\n","        [ 5.2446, -5.5963]], device='cuda:0', grad_fn=<AddmmBackward>), 'loss': tensor(1.7663, device='cuda:0', grad_fn=<NllLossBackward>)}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nLxhAVjIwo2g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}